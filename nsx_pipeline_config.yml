enable_ansible_debug: true # set value to true for verbose output from ansible

# format: "http://<jumphost_ip>:40001"
nsx_image_webserver: "CHANGE_ME" # http://10.61.96.18:40001 
#ova_file_name: "nsx-unified-appliance-2.2.0.0.0.8680778.ova" #Uncomment this if downloaded file manually and placed under /home/concourse
#ovftool_file_name: "VMware-ovftool-4.2.0-5965791-lin.x86_64.bundle"   #Uncomment this if downloaded file manually and placed under /home/concourse

# vCenter to deploy the NSX manager
vcenter_ip: CHANGE_ME         # 10.10.10.10
vcenter_username: CHANGE_ME   # vCenter username: vcenter_admin@yourdomain.com
vcenter_password: "CHANGE_ME" # "VMware1!"
vcenter_datacenter: CHANGE_ME # Datacenter where NSX will be deployed
vcenter_cluster: CHANGE_ME    # Cluster where NSX will be deployed 
vcenter_datastore: CHANGE_ME  # Datastore name where NSX will be deployed

# NSX manager general network settings
mgmt_portgroup: 'CHANGE_ME'   # Management portgroup 
dns_server: CHANGE_ME         # DNS server ip address
dns_domain: CHANGE_ME         # yourdomain.com
ntp_servers: CHANGE_ME        # NTP server ip or fqdn
default_gateway: CHANGE_ME    # Default gateway for Management network
netmask: CHANGE_ME            # Netmask for default gateway/mgmt network

nsx_manager_ip: CHANGE_ME                     # Ip address for nsx_manager on management network
nsx_manager_username: admin                   # Username for NSX manager
nsx_manager_password: Admin!23Admin           # NSX manager password
nsx_manager_assigned_hostname: "nsxt-manager" # this hostname+dns_domain will be FQDN and DNS record needed
nsx_manager_root_pwd: VMware1!                # Min 8 chars, upper, lower, number, special digit
nsx_manager_deployment_size: small            # Recommended for real barebones demo, smallest setup
nsx_manager_ssh_enabled: true
resource_reservation_off: true

# Compute manager credentials should be the same as above vCenter's if
# controllers and edges are to be on the same vCenter
compute_manager_username: "CHANGE_ME" # vCenter username: vcenter_admin@yourdomain.com
compute_manager_password: "VMware1!"
# compute manager for the compute cluster (2nd vCenter)
compute_manager_2_vcenter_ip: "null"
compute_manager_2_username: "null"
compute_manager_2_password: "null"

edge_uplink_profile_vlan: 0         # For outbound uplink connection used by Edge, usually keep as 0
esxi_uplink_profile_vlan: CHANGE_ME # For internal overlay connection used by Esxi hosts, usually trasnport VLAN ID

# Virtual Tunnel Endpoint network ip pool
vtep_ip_pool_cidr: CHANGE_ME        # CIDR for VTEP network vlan i.e - 172.15.0.0/24
vtep_ip_pool_gateway: CHANGE_ME     # Gateway to use for VTEP, usually first address in CIDR
vtep_ip_pool_start: CHANGE_ME       # Start of range for the VTEP, do not include gateway
vtep_ip_pool_end: CHANGE_ME         # End of range for the VTEP, do not include gateway

# Tier 0 router
tier0_router_name: T0RouterPKS1
tier0_uplink_port_ip: CHANGE_ME       # IP address for the first edge device from nsx_edge_uplink vlan
tier0_uplink_port_subnet: CHANGE_ME   # Number for the CIDR i.e 29 for the nsx_edge_uplink portgroup
tier0_uplink_next_hop_ip: CHANGE_ME   # IP address of physical router gateway
tier0_uplink_port_ip_2: CHANGE_ME     # IP address for the second edge uplink
tier0_ha_vip: CHANGE_ME               # IP address for the Edge HA VIP from the nsx_edge_uplink vlan

## Controllers (NSX Controller Setup)
controller_ips: CHANGE_ME                   # comma separated ip addresses from management network, usually 3
controller_default_gateway: CHANGE_ME       # Gateway for management network
controller_ip_prefix_length: CHANGE_ME      # CIDR for management network i.e. 26
controller_hostname_prefix: nsx-controller  # Name will be this + -# i.e - nsx-controller-01
controller_cli_password: "VMware1!"         # Min 8 chars, upper, lower, num, special char
controller_root_password: "VMware1!"        
controller_deployment_size: "SMALL"         
vc_datacenter_for_controller: CHANGE_ME           # Datacenter where NSX controllers will be installed
vc_cluster_for_controller: CHANGE_ME              # Cluster where NSX controllers will be installed
vc_datastore_for_controller: CHANGE_ME            # Datastore where nsx controller will be installed
vc_management_network_for_controller: "CHANGE_ME" # Management network portgroup
controller_shared_secret: "VMware1!VMware1!"    

## Edge nodes (Edge device setup)
edge_ips: CHANGE_ME                         # comma separated ip addresses from management network, usually 2
edge_default_gateway: CHANGE_ME             # Management network default gateway
edge_ip_prefix_length: CHANGE_ME            # Management network CIDR #, i.e - 29
edge_hostname_prefix: nsx-edge              # Edge vm name will be this + -# i.e nsx-edge-01
edge_transport_node_prefix: edge-tnode      # Edge transport node prefix seen in NSX Manager, i.e - edge-tnode-01
edge_cli_password: "VMware1!"
edge_root_password: "VMware1!"
edge_deployment_size: "large"               # Large recommended for PKS deployments
vc_datacenter_for_edge: CHANGE_ME           # Datacenter where edge VMs will be deployed
vc_cluster_for_edge: CHANGE_ME              # Cluster where edge VMs will be deployed
vc_datastore_for_edge: CHANGE_ME            # Datastore edge VMs will use
vc_uplink_network_for_edge: "CHANGE_ME"     # Edge uplink port group in vsphere: i.e - nsx_edge_uplink
vc_overlay_network_for_edge: "CHANGE_ME"    # TEP portgroup in vsphere (vSS or VDS .. must be set to higher >=1600)
vc_management_network_for_edge: "CHANGE_ME" # Management port group

## Import ESX hosts for NSX install 
# By auto_cluster (bring in all hosts from specified clusters)
# Can do multiple (comma separated)
# If multiple, each one matches up with the other
# I.e: 
#     clusters_to_install_nsx: cluster1,cluster2
#     per_cluster_vlans: 1001,1001
clusters_to_install_nsx: CHANGE_ME  # Name of cluster to bring in all hosts for NSX
per_cluster_vlans: 0                # TEP VLAN used for the uplink profile

# By host option, leave blank if doing by vcenter
esx_ips: ""             # additional esx hosts, if any, to be individually installed
esx_os_version: ""
esx_root_password: ""
esx_hostname_prefix: ""

# Free nic(s) on esx hosts prepared for nsx
esx_available_vmnic: "CHANGE_ME" # comma separated physical NICs.  Usually vmnic#

# Logical routers and switches
nsx_t_t1router_logical_switches_spec: |
  t1_routers:
  # Add additional T1 Routers or collapse switches into same T1 Router as needed
  - name: T1-PKS-MGMT
    switches:
    - name: LS-PKS-MGMT
      logical_switch_gw: CHANGE_ME # First address in the PKS-MGMT network, usually .1
      subnet_mask: CHANGE_ME       # Mask for the PKS-MGMT network, i.e - 26
  - name: T1-PKS-K8S
    switches:
    - name: LS-PKS-K8S
      logical_switch_gw: CHANGE_ME # Gateway for the VIP netowork
      subnet_mask: CHANGE_ME       # Mask for the VIP network, i.e - 24

# Non-routable networks inside of NSX
# Change to other non-routable /16 if desired
nsx_t_container_ip_block_spec: |
  container_ip_blocks:
  - name: pks-node-ip-block
    cidr: 11.4.0.0/16
  - name: pks-pod-ip-block
    cidr: 12.4.0.0/16


nsx_t_external_ip_pool_spec: |
  external_ip_pools:
  - name: pks-vip-pool
    cidr: CHANGE_ME     # CIDR for routable VIP network
    start: CHANGE_ME    # Start of range for routable VIP pool, should not include gateway
    end: CHANGE_ME      # End of range for routable VIP pool

# Leave unchanged unless PKS-MGMT network is non-routable
nsx_t_nat_rules_spec: |
  nat_rules:
  # Sample entry for  PKS-MGMT network, leave one uncommented that should be deleted
  - t0_router: T0RouterPKS1
    nat_type: snat
    source_network: 192.168.50.0/24      # PKS Infra network cidr
    translated_network: 10.208.50.3      # SNAT External Address for PKS networks
    rule_priority: 8001                  # Lower priority

  # Sample entry for PKS-K8s network
  #- t0_router: DefaultT0Router
  #  nat_type: snat
  #  source_network: 192.168.60.0/24      # PKS Clusters network cidr
  #  translated_network: 10.208.50.3      # SNAT External Address for PKS networks
  #  rule_priority: 8001                  # Lower priority

  # Sample entry for allowing inbound to PKS Ops manager
  #- t0_router: DefaultT0Router
  #  nat_type: dnat
  #  destination_network: 10.208.50.2     # External IP address for PKS opsmanager
  #  translated_network: 192.168.50.2     # Internal IP of PKS Ops manager
  #  rule_priority: 1024                  # Higher priority

  # Sample entry for allowing outbound from PKS Ops Mgr to external
  #- t0_router: DefaultT0Router
  #  nat_type: snat
  #  source_network: 192.168.50.2        # Internal IP of PAS opsmanager
  #  translated_network: 10.208.50.2      # External IP address for PAS opsmanager
  #  rule_priority: 1024                  # Higher priority

  # Sample entry for allowing inbound to PKS Controller
  #- t0_router: DefaultT0Router
  #  nat_type: dnat
  #  destination_network: 10.208.50.4     # External IP address for PKS opsmanager
  #  translated_network: 192.168.50.11     # Internal IP of PKS Ops Controller
  #  rule_priority: 1024                  # Higher priority

  # Sample entry for allowing outbound from PKS Controller to external
  #- t0_router: DefaultT0Router
  #  nat_type: snat
  #  source_network: 192.168.50.4        # Internal IP of PKS controller
  #  translated_network: 10.208.50.4      # External IP address for PKS controller
  #  rule_priority: 1024                  # Higher priority


nsx_t_csr_request_spec: |
  csr_request:
    #common_name not required - would use nsx_t_manager_host_name
    org_name: CHANGE_ME          # EDIT
    org_unit: CHANGE_ME          # EDIT
    country: US                  # EDIT
    state: CHANGE_ME             # EDIT
    city: CHANGE_ME              # EDIT
    key_size: 2048               # Valid values: 2048 or 3072
    algorithm: RSA               # Valid values: RSA or DSA


nsx_t_lbr_spec: |
  loadbalancers:
  
nsx_t_ha_switching_profile_spec: |
  ha_switching_profiles:


